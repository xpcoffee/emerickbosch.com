---
title: Inexpensive mechanisms to support team processes
description: A list of mechanisms that are cheap to implement, but that empower valuable processes.
faIcon: faPaperclip
date: 2022-01-22
tocDepth: 2
---

import { PullQuote } from "../components"

<div className="py-5" />

<span className="quicksand">
  This is a (hopefully) growing of mechanisms that are cheap to practically
  implement, but that empower processes that add value to the team and our
  customers.
</span>

<div className="py-5" />

# Team alignment

## Tenets

<p className="text-lg">
  A prioritized set of team defaults for how to resolve tension between
  incompatible requirements.
</p>

<PullQuote>Prioritize the things we care the most about.</PullQuote>

### Problem

We want to make correct decisions quickly and consistently, especially during times of high
stress.
This is really difficult to do if the team hasn't previously aligned on how to make
these difficult decisions.

### Concept

Pre-align the team around a set of default decisions/positions and use those as a starting point
when discussing tough decisions. Document these decisions/positions in a living document.
The resulting document is a list of tenets that can be used as a reference during times of
stress/contention to resolve arguments and make decisions quickly.
The document can also be shared with stakeholders and leadership to be more transparent
about how we make decisions and to give them an opportunity to give input/feedback.

**Very important!** Tenets are not rules.
Their primary goal is to help unblock _difficult_ decisions, which are generally ones where
the correct path is unclear in the moment.
There are many situations where we may make the opposite decision from what is in our tenets. Based on a context where the correct decision is clear.

**Examples**

```md
Team: API platform team

Tenets:

1.  Force multiplication over individual attention. We maximize our value by delivering
    on features that benefit the most amount of teams on the platform.
2.  General availability over individual availability. We protect the overall experience
    of the APIs on the platform; we will deteriorate the experience of individual APIs if
    their performance starts to affect other APIs.
3.  Developer experience over feature set. We first build and iterate on the experience
    of existing features such that they are safe and easy to integrate with before we
    consider additional features.
```

```md
Project: Address imminent scaling cliff

Tenets:

1.  Security over availability. We do not regress on our security posture, even if we end
    up hitting our scaling cliff.
2.  Results over perfection. The primary goal is to prevent our service from hitting the
    scaling cliff, even if that means taking on technical debt.
3.  Early warning over early delivery. We instrument our service to detect early signs of
    reaching the scaling cliff. We accept the risk of not making progress on the core
    deliverable for significant time up-front in favour of having the ability to detect
    critical failures early enough that we can react to them.
```

### Considerations

**Tenets should help resolve _contenious_ senarios.**

Tenets should themselves be contentious. Tenets that state the obvious bring little value.

```md
<!-- ❌ Useless; there is no tradeoff here -->

High quality over low quality. If we maintain high quality our customers will come back.

<!-- ✅ Better; clear tradeoff, position and rationale -->

Learning over usability. We push changes that may lower the usability of our service in
order to learn and deeper understand our customers.
```

**Tenets should be prioritized.**

It is very possible that tenets can be in tension with one another. Prioritizing them
prevents us from getting deadlocked between tenets.

This has the added benefit of communicating our values clearer to stakeholders.

**Tenets should change over time**

While some tenets can be based on a core philosophy that is stable (e.g. AWS has tenets around backwards compatibility)
most tenets are highly dependent on the context of the team. As the team changes due to growth of the system
, introduction of new responsibilities, or change in business goals so too should the tenets change.

**"It depends" isn't useful**

The point is to extract positions that can help make decisions quicker later.
Settling on answers that don't actually take a position isn't useful in this exercise.
Try to frame the senarios like "what do we choose if the decision is not clear from the context?"

## Tech experiment

A short-lived task aimed at gaining the information needed to make a decision.

<PullQuote>It is better to try, fail and learn than to stagnate.</PullQuote>

### Problem

Sometimes the team can't agree whether something is worth doing due to a lack of datapoints.

### Concept

We plan in time to implement a _minimal_ version _one_ of the options so that we can gather data and make progress.
We build metrics into the implementation that allow us to know whether the decision was correct,
and we plan an explicit deadline at which we evaluate the metrics and decide whether to plan the
full implementation or to roll back.

### Considerations

**Definitions of success, based on metrics**

The point of the experiment is to ultimately make a decision. To make that as easy as possible,
success metrics/conditions need to be well defined up-front and baked into the implementation.

**2 way doors**

The implementation must to be _easily_ reversible.
This helps mitigate the risk we take when making a decision on limited data.

**Priority still matters**

In some cases it can be useful to decide to reduce the scope of a feature to an experiment
to allow it to be pulled in when there is low capacity, but this can also be abused. Be
conscious of experiments "jumping the queue" in the backlog due to their light weight.

**Not the same as an investigation/spike**

Investigations/spikes are focused on disambiguating **how** to build something and to build
knowledge. They do not necessarily require any implementation and they can still be necessary
after a decision has been made e.g. we know what feature to build but we need to figure out
some implementation details.

In contrast an experiment is about trying something "for real" and measuring the results to
enable us to make a decision.

### Experiment proposal

This is a template for an experiment proposal/overview. This is used to make the initial decision around
whether or not to plan the experiment.

```md
# Experiment: Foobar

## Context

How do things work now?

## Ambiguity/problem

What knowledge are we aiming to gain/what problem are we looking to solve with this
experiment?

## Approach

Short description of the approach. This is not a design; that will come later.

## Out of scope

What will not be considered as part of this experiment.

## Success criteria/Metrics

How do we know whether this was successful or not?

## Owners

@someone
@anyone

## Start date

YYYY-MM-DD

## End date

YYYY-MM-DD

This is a hard cutoff. On this date we make a decision around failure/success and
whether to invest further.

## Conclusions/learnings

To be written after the experiment to summaries the outcomes. Link to more detailed
artifacts here (designs, data, code, etc).
```

<div className="py-5" />

# Operations

## Operational event log

Team-specific log of start/end of significant operational events. Most recent entries
at the top.

<PullQuote>A reference of the most important events in our systems.</PullQuote>

### Problem

It is time-consuming and often difficult to find and correlate events that happened
in systems that we're dependent on or in systems that are dependent on us, especially
if these events weren't widely-spread or shared.

This difficulty means that you usually need to manually reach out to people involved in
incidents in order to find out what happened.

### Concept

Teams keep a minimal log of the most important events that happen to their systems.
This log can be accessed by other teams that want to correlate events they see in their own
systems or by team mates in the future to determine if there are patterns in events.

The logs are also used as major time-markers in post-mortem investigations for larger incidents
to understand impact and recovery across multiple systems.

In more advanced cases these logs should be able to be injested by tooling to display
events across the company on dashboards or to run analytics on the data.

### Ops event log format

This format tracks major state changes (when an event starts, major evolutions and when an event ends).
There can be one log per team or multiple teams can contribute to a shared log.

```md
timestamp | short event description | status (start/state-change/end) | _ticket id_ | short status change description
```

Example:

```md
# Operational events

2021-08-30T10:14Z | home page latency increase | end | HOME-8557 | full recovery
2021-08-30T10:13Z | home page latency increase | state-change | HOME-8557 | partial recovery
2021-08-30T10:11Z | home page latency increase | start | HOME-8557 | alarm fires
2021-08-29T12:00Z | failover data migration | end | DATA-8552 | workflow successful
2021-08-29T09:31Z | failover data migration | start | DATA-8552 | workflow started
```

## Operational review journal

Team-specific journal of a periodic operational review.

<PullQuote>How did our systems perform for our customers last week?</PullQuote>

**What does this promote?**

Knowledge sharing on operational events / historical records of operational events / data gathering of operational events / data analysis of operational events / record of operational issues / record of operational decisions.

**When do you use it?**

Every operational review session.

**Why is it useful?**

Record of the review for tracking, knowledge-sharing, reference and long-term trend analysis.

```md
# YYYY-MM-DD Ops review

## What experiences do we own/maintain?

_This section shouldn't change often and is more to allow sharing._

## Operational wins

## What was last week like for customers?

Customer impacting events (both good and bad):

Customer feedback:

Metrics callouts:

## What was last week like for on-call?

Major events:

Notable classes of issues:

Lessons learned:

Most wanted improvement:

## What is currently the biggest operational risk?

## Action items

1. Action item - @owner
```
